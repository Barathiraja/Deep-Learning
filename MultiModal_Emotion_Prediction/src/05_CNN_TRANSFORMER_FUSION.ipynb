{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47f42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:\n",
      "  EDA : (281, 60)\n",
      "  BEH : (281, 5)\n",
      "  Labels: (281,)\n",
      "âœ“ Labels remapped to: [0 1 2]\n",
      "âœ“ Normalization complete\n",
      "Train batches: 7 | Test batches: 2\n",
      "  Batch 5/7 | Loss: 1.0385\n",
      "Epoch 1/30 | Loss: 7.2972 | Time: 2.4s\n",
      "  Batch 5/7 | Loss: 0.9896\n",
      "Epoch 2/30 | Loss: 6.6079 | Time: 2.4s\n",
      "  Batch 5/7 | Loss: 0.7194\n",
      "Epoch 3/30 | Loss: 5.7154 | Time: 2.5s\n",
      "  Batch 5/7 | Loss: 0.6667\n",
      "Epoch 4/30 | Loss: 5.2556 | Time: 2.5s\n",
      "  Batch 5/7 | Loss: 0.6536\n",
      "Epoch 5/30 | Loss: 4.4212 | Time: 2.4s\n",
      "  Batch 5/7 | Loss: 0.4402\n",
      "Epoch 6/30 | Loss: 3.8225 | Time: 2.6s\n",
      "  Batch 5/7 | Loss: 0.4109\n",
      "Epoch 7/30 | Loss: 3.2188 | Time: 3.4s\n",
      "  Batch 5/7 | Loss: 0.4134\n",
      "Epoch 8/30 | Loss: 2.8382 | Time: 3.3s\n",
      "  Batch 5/7 | Loss: 0.4399\n",
      "Epoch 9/30 | Loss: 2.5538 | Time: 4.1s\n",
      "  Batch 5/7 | Loss: 0.4142\n",
      "Epoch 10/30 | Loss: 2.2546 | Time: 3.8s\n",
      "ðŸ”“ Encoders unfrozen\n",
      "  Batch 5/7 | Loss: 0.4990\n",
      "Epoch 11/30 | Loss: 3.3907 | Time: 5.2s\n",
      "  Batch 5/7 | Loss: 0.3281\n",
      "Epoch 12/30 | Loss: 2.3510 | Time: 5.1s\n",
      "  Batch 5/7 | Loss: 0.4226\n",
      "Epoch 13/30 | Loss: 2.1161 | Time: 4.9s\n",
      "  Batch 5/7 | Loss: 0.2333\n",
      "Epoch 14/30 | Loss: 2.0842 | Time: 4.9s\n",
      "  Batch 5/7 | Loss: 0.5690\n",
      "Epoch 15/30 | Loss: 2.2154 | Time: 4.8s\n",
      "  Batch 5/7 | Loss: 0.3476\n",
      "Epoch 16/30 | Loss: 1.8931 | Time: 4.9s\n",
      "  Batch 5/7 | Loss: 0.1494\n",
      "Epoch 17/30 | Loss: 1.6618 | Time: 4.9s\n",
      "  Batch 5/7 | Loss: 0.2204\n",
      "Epoch 18/30 | Loss: 1.5165 | Time: 4.9s\n",
      "  Batch 5/7 | Loss: 0.1167\n",
      "Epoch 19/30 | Loss: 1.3673 | Time: 5.1s\n",
      "  Batch 5/7 | Loss: 0.2777\n",
      "Epoch 20/30 | Loss: 1.4641 | Time: 5.0s\n",
      "  Batch 5/7 | Loss: 0.1685\n",
      "Epoch 21/30 | Loss: 1.2264 | Time: 4.9s\n",
      "  Batch 5/7 | Loss: 0.1056\n",
      "Epoch 22/30 | Loss: 1.0948 | Time: 5.0s\n",
      "  Batch 5/7 | Loss: 0.1138\n",
      "Epoch 23/30 | Loss: 1.0428 | Time: 5.0s\n",
      "  Batch 5/7 | Loss: 0.1619\n",
      "Epoch 24/30 | Loss: 1.0910 | Time: 4.8s\n",
      "  Batch 5/7 | Loss: 0.1032\n",
      "Epoch 25/30 | Loss: 0.8670 | Time: 5.0s\n",
      "  Batch 5/7 | Loss: 0.1010\n",
      "Epoch 26/30 | Loss: 1.0426 | Time: 5.0s\n",
      "  Batch 5/7 | Loss: 0.1212\n",
      "Epoch 27/30 | Loss: 0.8899 | Time: 4.8s\n",
      "  Batch 5/7 | Loss: 0.2626\n",
      "Epoch 28/30 | Loss: 1.0460 | Time: 5.0s\n",
      "  Batch 5/7 | Loss: 0.1900\n",
      "Epoch 29/30 | Loss: 0.8642 | Time: 5.1s\n",
      "  Batch 5/7 | Loss: 0.1398\n",
      "Epoch 30/30 | Loss: 1.2516 | Time: 4.9s\n",
      "\n",
      "ðŸ“Š Evaluation Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.91        31\n",
      "           1       1.00      1.00      1.00        17\n",
      "           2       0.64      1.00      0.78         9\n",
      "\n",
      "    accuracy                           0.91        57\n",
      "   macro avg       0.88      0.95      0.90        57\n",
      "weighted avg       0.94      0.91      0.92        57\n",
      "\n",
      "[[26  0  5]\n",
      " [ 0 17  0]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# STEP 5 â€” CNN + Temporal Transformer + Behavioral Fusion\n",
    "# (WITH TRAINING LOGS)\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# Load Step-3 engineered data\n",
    "# -----------------------------\n",
    "data = np.load(\"features_S2_w60.npz\")\n",
    "\n",
    "EDA  = data[\"EDA_windows\"]\n",
    "BVP  = data[\"BVP_windows\"]\n",
    "ACC  = data[\"ACC_windows\"]\n",
    "TEMP = data[\"TEMP_windows\"]\n",
    "BEH  = data[\"behavior_features\"]\n",
    "labels = data[\"labels\"]\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(\"  EDA :\", EDA.shape)\n",
    "print(\"  BEH :\", BEH.shape)\n",
    "print(\"  Labels:\", labels.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Label remap\n",
    "# -----------------------------\n",
    "labels = labels - 1\n",
    "assert labels.min() == 0 and labels.max() == 2\n",
    "print(\"âœ“ Labels remapped to:\", np.unique(labels))\n",
    "\n",
    "# -----------------------------\n",
    "# Normalization\n",
    "# -----------------------------\n",
    "def z_norm_windowwise(x):\n",
    "    return (x - x.mean(axis=1, keepdims=True)) / (x.std(axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "EDA  = z_norm_windowwise(EDA)\n",
    "BVP  = z_norm_windowwise(BVP)\n",
    "ACC  = z_norm_windowwise(ACC)\n",
    "TEMP = z_norm_windowwise(TEMP)\n",
    "\n",
    "BEH = (BEH - BEH.mean(axis=0)) / (BEH.std(axis=0) + 1e-6)\n",
    "\n",
    "print(\" Normalization complete\")\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class WESADDataset(Dataset):\n",
    "    def __init__(self, EDA, BVP, ACC, TEMP, BEH, y):\n",
    "        self.EDA  = torch.tensor(EDA,  dtype=torch.float32).unsqueeze(-1)\n",
    "        self.BVP  = torch.tensor(BVP,  dtype=torch.float32).unsqueeze(-1)\n",
    "        self.ACC  = torch.tensor(ACC,  dtype=torch.float32).unsqueeze(-1)\n",
    "        self.TEMP = torch.tensor(TEMP, dtype=torch.float32).unsqueeze(-1)\n",
    "        self.BEH  = torch.tensor(BEH,  dtype=torch.float32)\n",
    "        self.y    = torch.tensor(y,    dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.EDA[idx], self.BVP[idx], self.ACC[idx], self.TEMP[idx], self.BEH[idx], self.y[idx]\n",
    "\n",
    "# -----------------------------\n",
    "# Train / Test split\n",
    "# -----------------------------\n",
    "idx = np.arange(len(labels))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    idx, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "train_ds = WESADDataset(\n",
    "    EDA[train_idx], BVP[train_idx], ACC[train_idx],\n",
    "    TEMP[train_idx], BEH[train_idx], labels[train_idx]\n",
    ")\n",
    "\n",
    "test_ds = WESADDataset(\n",
    "    EDA[test_idx], BVP[test_idx], ACC[test_idx],\n",
    "    TEMP[test_idx], BEH[test_idx], labels[test_idx]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Test batches: {len(test_loader)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Model components\n",
    "# -----------------------------\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, dim, 3, padding=1),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.permute(0,2,1)).permute(0,2,1)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(dim, nhead=4, batch_first=True)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=1)  # ðŸ”¥ faster\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.enc(x)\n",
    "\n",
    "class BehaviorEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(dim, nhead=4, batch_first=True)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=1)  # ðŸ”¥ faster\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.enc(x)\n",
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, beh_dim, dim=128):\n",
    "        super().__init__()\n",
    "        self.cnn = CNNEncoder(dim)\n",
    "        self.temp_tf = TemporalTransformer(dim)\n",
    "        self.beh_enc = BehaviorEncoder(beh_dim, dim)\n",
    "        self.fuse_tf = FusionTransformer(dim)\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.temp_tf(self.cnn(x)).mean(dim=1)\n",
    "\n",
    "    def forward(self, EDA, BVP, ACC, TEMP, BEH):\n",
    "        feats = [\n",
    "            self.encode(EDA),\n",
    "            self.encode(BVP),\n",
    "            self.encode(ACC),\n",
    "            self.encode(TEMP),\n",
    "            self.beh_enc(BEH)\n",
    "        ]\n",
    "        fused = self.fuse_tf(torch.stack(feats, dim=1)).mean(dim=1)\n",
    "        return self.cls(fused)\n",
    "\n",
    "# -----------------------------\n",
    "# Training setup\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmotionModel(BEH.shape[1]).to(device)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0,1,2]),\n",
    "    y=labels\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Freeze encoders\n",
    "for p in model.cnn.parameters(): p.requires_grad = False\n",
    "for p in model.temp_tf.parameters(): p.requires_grad = False\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop (LOGGED)\n",
    "# -----------------------------\n",
    "EPOCHS = 30\n",
    "LOG_INTERVAL = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    if epoch == 10:\n",
    "        for p in model.cnn.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in model.temp_tf.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    print(\"Encoders unfrozen\")\n",
    "\n",
    "    for i, (EDA,BVP,ACC,TEMP,BEH,y) in enumerate(train_loader):\n",
    "        EDA,BVP,ACC,TEMP,BEH,y = (\n",
    "            EDA.to(device), BVP.to(device),\n",
    "            ACC.to(device), TEMP.to(device),\n",
    "            BEH.to(device), y.to(device)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(EDA,BVP,ACC,TEMP,BEH), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (i+1) % LOG_INTERVAL == 0:\n",
    "            print(f\"  Batch {i+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Loss: {epoch_loss:.4f} | \"\n",
    "        f\"Time: {(time.time()-start_time):.1f}s\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "preds, gts = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for EDA,BVP,ACC,TEMP,BEH,y in test_loader:\n",
    "        out = model(\n",
    "            EDA.to(device), BVP.to(device),\n",
    "            ACC.to(device), TEMP.to(device),\n",
    "            BEH.to(device)\n",
    "        )\n",
    "        preds.extend(torch.argmax(out,1).cpu().numpy())\n",
    "        gts.extend(y.numpy())\n",
    "\n",
    "print(\"Evaluation Results\")\n",
    "print(classification_report(gts, preds, zero_division=0))\n",
    "print(confusion_matrix(gts, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2233889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Multi-subject data loaded\n",
      "Total samples: 1440\n",
      "Label distribution: (array([0, 1, 2]), array([774, 423, 243]))\n",
      "âœ“ Normalization complete\n",
      "Behavior feature dim: 4\n",
      "Train batches: 36 | Test batches: 9\n",
      "  Batch 5/36 | Loss: 1.1017\n",
      "  Batch 10/36 | Loss: 1.2869\n",
      "  Batch 15/36 | Loss: 1.0733\n",
      "  Batch 20/36 | Loss: 1.0201\n",
      "  Batch 25/36 | Loss: 0.9568\n",
      "  Batch 30/36 | Loss: 0.9126\n",
      "  Batch 35/36 | Loss: 0.8698\n",
      "Epoch 1/30 | Loss: 36.4103 | Time: 35.6s\n",
      "  Batch 5/36 | Loss: 0.8306\n",
      "  Batch 10/36 | Loss: 0.7050\n",
      "  Batch 15/36 | Loss: 0.6046\n",
      "  Batch 20/36 | Loss: 0.8174\n",
      "  Batch 25/36 | Loss: 0.8776\n",
      "  Batch 30/36 | Loss: 0.7501\n",
      "  Batch 35/36 | Loss: 0.8499\n",
      "Epoch 2/30 | Loss: 27.9356 | Time: 32.0s\n",
      "  Batch 5/36 | Loss: 0.9751\n",
      "  Batch 10/36 | Loss: 0.6803\n",
      "  Batch 15/36 | Loss: 0.6046\n",
      "  Batch 20/36 | Loss: 0.6073\n",
      "  Batch 25/36 | Loss: 0.5842\n",
      "  Batch 30/36 | Loss: 0.7182\n",
      "  Batch 35/36 | Loss: 0.7018\n",
      "Epoch 3/30 | Loss: 26.1420 | Time: 29.0s\n",
      "  Batch 5/36 | Loss: 0.7398\n",
      "  Batch 10/36 | Loss: 0.5630\n",
      "  Batch 15/36 | Loss: 0.4308\n",
      "  Batch 20/36 | Loss: 0.6323\n",
      "  Batch 25/36 | Loss: 0.5175\n",
      "  Batch 30/36 | Loss: 0.4859\n",
      "  Batch 35/36 | Loss: 0.8035\n",
      "Epoch 4/30 | Loss: 23.7233 | Time: 27.2s\n",
      "  Batch 5/36 | Loss: 0.2996\n",
      "  Batch 10/36 | Loss: 0.3484\n",
      "  Batch 15/36 | Loss: 0.4086\n",
      "  Batch 20/36 | Loss: 0.6003\n",
      "  Batch 25/36 | Loss: 0.8447\n",
      "  Batch 30/36 | Loss: 0.6537\n",
      "  Batch 35/36 | Loss: 0.5324\n",
      "Epoch 5/30 | Loss: 20.9279 | Time: 27.7s\n",
      "  Batch 5/36 | Loss: 0.6110\n",
      "  Batch 10/36 | Loss: 0.4776\n",
      "  Batch 15/36 | Loss: 0.4978\n",
      "  Batch 20/36 | Loss: 0.4925\n",
      "  Batch 25/36 | Loss: 0.3950\n",
      "  Batch 30/36 | Loss: 0.4948\n",
      "  Batch 35/36 | Loss: 0.5079\n",
      "Epoch 6/30 | Loss: 19.8765 | Time: 27.8s\n",
      "  Batch 5/36 | Loss: 0.3296\n",
      "  Batch 10/36 | Loss: 0.4192\n",
      "  Batch 15/36 | Loss: 0.4160\n",
      "  Batch 20/36 | Loss: 0.3454\n",
      "  Batch 25/36 | Loss: 0.4238\n",
      "  Batch 30/36 | Loss: 0.7029\n",
      "  Batch 35/36 | Loss: 0.3361\n",
      "Epoch 7/30 | Loss: 17.5150 | Time: 28.2s\n",
      "  Batch 5/36 | Loss: 0.5027\n",
      "  Batch 10/36 | Loss: 0.5292\n",
      "  Batch 15/36 | Loss: 0.2832\n",
      "  Batch 20/36 | Loss: 0.3359\n",
      "  Batch 25/36 | Loss: 0.5600\n",
      "  Batch 30/36 | Loss: 0.4801\n",
      "  Batch 35/36 | Loss: 0.5488\n",
      "Epoch 8/30 | Loss: 17.0058 | Time: 28.8s\n",
      "  Batch 5/36 | Loss: 0.5119\n",
      "  Batch 10/36 | Loss: 0.3348\n",
      "  Batch 15/36 | Loss: 0.3877\n",
      "  Batch 20/36 | Loss: 0.1853\n",
      "  Batch 25/36 | Loss: 0.1822\n",
      "  Batch 30/36 | Loss: 0.4819\n",
      "  Batch 35/36 | Loss: 0.5751\n",
      "Epoch 9/30 | Loss: 14.8801 | Time: 38.6s\n",
      "  Batch 5/36 | Loss: 0.3688\n",
      "  Batch 10/36 | Loss: 0.3559\n",
      "  Batch 15/36 | Loss: 0.2968\n",
      "  Batch 20/36 | Loss: 0.2358\n",
      "  Batch 25/36 | Loss: 0.4660\n",
      "  Batch 30/36 | Loss: 0.6770\n",
      "  Batch 35/36 | Loss: 0.2062\n",
      "Epoch 10/30 | Loss: 12.5452 | Time: 31.6s\n",
      "ðŸ”“ Encoders unfrozen\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP-6 â€” Multi-Subject Training (Physio + Behavioral Fusion)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "SUBJECT_FILES = [\n",
    "    \"features_S2_w60.npz\",\n",
    "    \"features_S3_w60.npz\",\n",
    "    \"features_S4_w60.npz\",\n",
    "    \"features_S5_w60.npz\",\n",
    "    \"features_S6_w60.npz\",\n",
    "    \"features_S7_w60.npz\",\n",
    "    \"features_S8_w60.npz\",\n",
    "    \"features_S9_w60.npz\",\n",
    "    \"features_S10_w60.npz\",\n",
    "    \"features_S11_w60.npz\",\n",
    "    \"features_S12_w60.npz\",\n",
    "    \"features_S13_w60.npz\",\n",
    "    \"features_S14_w60.npz\",\n",
    "    \"features_S15_w60.npz\",\n",
    "    \"features_S17_w60.npz\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD & MERGE SUBJECT DATA\n",
    "# -----------------------------\n",
    "EDA_all, BVP_all, ACC_all, TEMP_all, BEH_all, Y_all = [], [], [], [], [], []\n",
    "\n",
    "for path in SUBJECT_FILES:\n",
    "    d = np.load(path)\n",
    "\n",
    "    EDA_all.append(d[\"EDA_windows\"])\n",
    "    BVP_all.append(d[\"BVP_windows\"])\n",
    "    ACC_all.append(d[\"ACC_windows\"])\n",
    "    TEMP_all.append(d[\"TEMP_windows\"])\n",
    "    BEH_all.append(d[\"behavior_features\"])\n",
    "\n",
    "    # ðŸ”‘ label fix: {1,2,3} â†’ {0,1,2}\n",
    "    y = d[\"labels\"] - 1\n",
    "    assert y.min() == 0 and y.max() == 2\n",
    "    Y_all.append(y)\n",
    "\n",
    "EDA_all  = np.concatenate(EDA_all)\n",
    "BVP_all  = np.concatenate(BVP_all)\n",
    "ACC_all  = np.concatenate(ACC_all)\n",
    "TEMP_all = np.concatenate(TEMP_all)\n",
    "BEH_all  = np.concatenate(BEH_all)\n",
    "Y_all    = np.concatenate(Y_all)\n",
    "\n",
    "print(\"âœ“ Multi-subject data loaded\")\n",
    "print(\"Total samples:\", len(Y_all))\n",
    "print(\"Label distribution:\", np.unique(Y_all, return_counts=True))\n",
    "\n",
    "# -----------------------------\n",
    "# NORMALIZATION\n",
    "# -----------------------------\n",
    "def z_norm_windowwise(x):\n",
    "    mean = x.mean(axis=1, keepdims=True)\n",
    "    std  = x.std(axis=1, keepdims=True) + 1e-6\n",
    "    return (x - mean) / std\n",
    "\n",
    "EDA_all  = z_norm_windowwise(EDA_all)\n",
    "BVP_all  = z_norm_windowwise(BVP_all)\n",
    "ACC_all  = z_norm_windowwise(ACC_all)\n",
    "TEMP_all = z_norm_windowwise(TEMP_all)\n",
    "\n",
    "BEH_all = (BEH_all - BEH_all.mean(axis=0)) / (BEH_all.std(axis=0) + 1e-6)\n",
    "\n",
    "print(\"âœ“ Normalization complete\")\n",
    "print(\"Behavior feature dim:\", BEH_all.shape[1])\n",
    "\n",
    "# -----------------------------\n",
    "# DATASET\n",
    "# -----------------------------\n",
    "class WESADDataset(Dataset):\n",
    "    def __init__(self, EDA, BVP, ACC, TEMP, BEH, labels):\n",
    "        self.EDA  = torch.tensor(EDA,  dtype=torch.float32).unsqueeze(-1)\n",
    "        self.BVP  = torch.tensor(BVP,  dtype=torch.float32).unsqueeze(-1)\n",
    "        self.ACC  = torch.tensor(ACC,  dtype=torch.float32).unsqueeze(-1)\n",
    "        self.TEMP = torch.tensor(TEMP, dtype=torch.float32).unsqueeze(-1)\n",
    "        self.BEH  = torch.tensor(BEH,  dtype=torch.float32)\n",
    "        self.y    = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.EDA[idx], self.BVP[idx], self.ACC[idx], self.TEMP[idx], self.BEH[idx], self.y[idx]\n",
    "\n",
    "# -----------------------------\n",
    "# TRAIN / TEST SPLIT\n",
    "# -----------------------------\n",
    "idx = np.arange(len(Y_all))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    idx, test_size=0.2, stratify=Y_all, random_state=42\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    WESADDataset(\n",
    "        EDA_all[train_idx], BVP_all[train_idx], ACC_all[train_idx],\n",
    "        TEMP_all[train_idx], BEH_all[train_idx], Y_all[train_idx]\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    WESADDataset(\n",
    "        EDA_all[test_idx], BVP_all[test_idx], ACC_all[test_idx],\n",
    "        TEMP_all[test_idx], BEH_all[test_idx], Y_all[test_idx]\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Test batches: {len(test_loader)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL DEFINITIONS\n",
    "# -----------------------------\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, in_ch=1, dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 32, 5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, dim, 3, padding=1),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return self.net(x).permute(0, 2, 1)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(d_model=dim, nhead=4, batch_first=True)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.enc(x)\n",
    "\n",
    "class BehaviorEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(d_model=dim, nhead=4, batch_first=True)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.enc(x)\n",
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, beh_dim, dim=128, classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = CNNEncoder(1, dim)\n",
    "        self.temp_tf = TemporalTransformer(dim)\n",
    "        self.beh_enc = BehaviorEncoder(beh_dim, dim)\n",
    "        self.fuse_tf = FusionTransformer(dim)\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, classes)\n",
    "        )\n",
    "\n",
    "    def encode_signal(self, x):\n",
    "        return self.temp_tf(self.cnn(x)).mean(dim=1)\n",
    "\n",
    "    def forward(self, EDA, BVP, ACC, TEMP, BEH):\n",
    "        e1 = self.encode_signal(EDA)\n",
    "        e2 = self.encode_signal(BVP)\n",
    "        e3 = self.encode_signal(ACC)\n",
    "        e4 = self.encode_signal(TEMP)\n",
    "        eb = self.beh_enc(BEH)\n",
    "\n",
    "        fused = torch.stack([e1, e2, e3, e4, eb], dim=1)\n",
    "        fused = self.fuse_tf(fused).mean(dim=1)\n",
    "        return self.cls(fused)\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING SETUP \n",
    "# -----------------------------\n",
    "beh_dim = BEH_all.shape[1]\n",
    "model = EmotionModel(beh_dim=beh_dim).to(device)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0, 1, 2]),\n",
    "    y=Y_all\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Freeze encoders initially\n",
    "for p in model.cnn.parameters():     p.requires_grad = False\n",
    "for p in model.temp_tf.parameters(): p.requires_grad = False\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    if epoch == 10:\n",
    "        for p in model.cnn.parameters():     p.requires_grad = True\n",
    "        for p in model.temp_tf.parameters(): p.requires_grad = True\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        print(\"ðŸ”“ Encoders unfrozen\")\n",
    "\n",
    "    for i, batch in enumerate(train_loader, 1):\n",
    "        EDA, BVP, ACC, TEMP, BEH, y = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(EDA, BVP, ACC, TEMP, BEH), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f\"  Batch {i}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.4f} | Time: {time.time()-start:.1f}s\")\n",
    "\n",
    "# -----------------------------\n",
    "# EVALUATION\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "preds, gts = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        EDA, BVP, ACC, TEMP, BEH, y = batch\n",
    "        out = model(\n",
    "            EDA.to(device), BVP.to(device),\n",
    "            ACC.to(device), TEMP.to(device),\n",
    "            BEH.to(device)\n",
    "        )\n",
    "        preds.extend(out.argmax(1).cpu().numpy())\n",
    "        gts.extend(y.numpy())\n",
    "\n",
    "print(\"\\nðŸ“Š Multi-Subject Evaluation\")\n",
    "print(classification_report(gts, preds, zero_division=0))\n",
    "print(confusion_matrix(gts, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b8422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª LOSO Test Subject: features_S2_w60.npz\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       152\n",
      "           1       0.29      1.00      0.45        82\n",
      "           2       0.00      0.00      0.00        47\n",
      "\n",
      "    accuracy                           0.29       281\n",
      "   macro avg       0.10      0.33      0.15       281\n",
      "weighted avg       0.09      0.29      0.13       281\n",
      "\n",
      "\n",
      "ðŸ§ª LOSO Test Subject: features_S3_w60.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Barathirajan M\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Barathirajan M\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Barathirajan M\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.95      0.69       152\n",
      "           1       0.00      0.00      0.00        85\n",
      "           2       0.00      0.00      0.00        49\n",
      "\n",
      "    accuracy                           0.51       286\n",
      "   macro avg       0.18      0.32      0.23       286\n",
      "weighted avg       0.29      0.51      0.37       286\n",
      "\n",
      "\n",
      "ðŸ§ª LOSO Test Subject: features_S4_w60.npz\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.29      1.00      0.45        84\n",
      "           2       0.00      0.00      0.00        49\n",
      "\n",
      "    accuracy                           0.29       287\n",
      "   macro avg       0.10      0.33      0.15       287\n",
      "weighted avg       0.09      0.29      0.13       287\n",
      "\n",
      "\n",
      "ðŸ§ª LOSO Test Subject: features_S5_w60.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Barathirajan M\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Barathirajan M\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Barathirajan M\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       159\n",
      "           1       0.29      1.00      0.45        85\n",
      "           2       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.29       294\n",
      "   macro avg       0.10      0.33      0.15       294\n",
      "weighted avg       0.08      0.29      0.13       294\n",
      "\n",
      "\n",
      "ðŸ§ª LOSO Test Subject: features_S6_w60.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Barathirajan M\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Barathirajan M\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Barathirajan M\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.70      0.64       157\n",
      "           1       0.11      0.14      0.12        87\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.42       292\n",
      "   macro avg       0.24      0.28      0.26       292\n",
      "weighted avg       0.35      0.42      0.38       292\n",
      "\n",
      "\n",
      "âœ… LOSO Validation Completed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP-7 â€” LOSO VALIDATION (Physio + Behavioral Fusion)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "SUBJECT_FILES = [\n",
    "    \"features_S2_w60.npz\",\n",
    "    \"features_S3_w60.npz\",\n",
    "    \"features_S4_w60.npz\",\n",
    "    \"features_S5_w60.npz\",\n",
    "    \"features_S6_w60.npz\",\n",
    "    \"features_S7_w60.npz\",\n",
    "    \"features_S8_w60.npz\",\n",
    "    \"features_S9_w60.npz\",\n",
    "    \"features_S10_w60.npz\",\n",
    "    \"features_S11_w60.npz\",\n",
    "    \"features_S12_w60.npz\",\n",
    "    \"features_S13_w60.npz\",\n",
    "    \"features_S14_w60.npz\",\n",
    "    \"features_S15_w60.npz\",\n",
    "    \"features_S17_w60.npz\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# DATASET\n",
    "# -----------------------------\n",
    "class WESADDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, EDA, BVP, ACC, TEMP, BEH, labels):\n",
    "        self.EDA  = torch.tensor(EDA, dtype=torch.float32).unsqueeze(-1)\n",
    "        self.BVP  = torch.tensor(BVP, dtype=torch.float32).unsqueeze(-1)\n",
    "        self.ACC  = torch.tensor(ACC, dtype=torch.float32).unsqueeze(-1)\n",
    "        self.TEMP = torch.tensor(TEMP, dtype=torch.float32).unsqueeze(-1)\n",
    "        self.BEH  = torch.tensor(BEH, dtype=torch.float32)\n",
    "        self.y    = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.EDA[idx], self.BVP[idx], self.ACC[idx], self.TEMP[idx], self.BEH[idx], self.y[idx]\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL DEFINITIONS\n",
    "# -----------------------------\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, in_ch=1, dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 32, 5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, dim, 3, padding=1),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.permute(0,2,1)).permute(0,2,1)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(d_model=dim, nhead=4, batch_first=True)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.enc(x)\n",
    "\n",
    "class BehaviorEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(d_model=dim, nhead=4, batch_first=True)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.enc(x)\n",
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, beh_dim, dim=128, classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = CNNEncoder(1, dim)\n",
    "        self.temp_tf = TemporalTransformer(dim)\n",
    "        self.beh_enc = BehaviorEncoder(beh_dim, dim)\n",
    "        self.fuse_tf = FusionTransformer(dim)\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, classes)\n",
    "        )\n",
    "\n",
    "    def encode_signal(self, x):\n",
    "        return self.temp_tf(self.cnn(x)).mean(dim=1)\n",
    "\n",
    "    def forward(self, EDA, BVP, ACC, TEMP, BEH):\n",
    "        e1 = self.encode_signal(EDA)\n",
    "        e2 = self.encode_signal(BVP)\n",
    "        e3 = self.encode_signal(ACC)\n",
    "        e4 = self.encode_signal(TEMP)\n",
    "        eb = self.beh_enc(BEH)\n",
    "\n",
    "        fused = torch.stack([e1, e2, e3, e4, eb], dim=1)\n",
    "        fused = self.fuse_tf(fused).mean(dim=1)\n",
    "        return self.cls(fused)\n",
    "\n",
    "# -----------------------------\n",
    "# LOSO LOOP\n",
    "# -----------------------------\n",
    "all_reports = []\n",
    "\n",
    "for test_subject in SUBJECT_FILES:\n",
    "    print(f\"\\nðŸ§ª LOSO Test Subject: {test_subject}\")\n",
    "\n",
    "    # Load test subject\n",
    "    test_data = np.load(test_subject)\n",
    "\n",
    "    EDA_test  = test_data[\"EDA_windows\"]\n",
    "    BVP_test  = test_data[\"BVP_windows\"]\n",
    "    ACC_test  = test_data[\"ACC_windows\"]\n",
    "    TEMP_test = test_data[\"TEMP_windows\"]\n",
    "    BEH_test  = test_data[\"behavior_features\"]\n",
    "    y_test    = test_data[\"labels\"] - 1\n",
    "\n",
    "    # Load training subjects\n",
    "    EDA_tr, BVP_tr, ACC_tr, TEMP_tr, BEH_tr, y_tr = [], [], [], [], [], []\n",
    "\n",
    "    for train_subject in SUBJECT_FILES:\n",
    "        if train_subject == test_subject:\n",
    "            continue\n",
    "        d = np.load(train_subject)\n",
    "        EDA_tr.append(d[\"EDA_windows\"])\n",
    "        BVP_tr.append(d[\"BVP_windows\"])\n",
    "        ACC_tr.append(d[\"ACC_windows\"])\n",
    "        TEMP_tr.append(d[\"TEMP_windows\"])\n",
    "        BEH_tr.append(d[\"behavior_features\"])\n",
    "        y_tr.append(d[\"labels\"] - 1)\n",
    "\n",
    "    EDA_tr  = np.concatenate(EDA_tr)\n",
    "    BVP_tr  = np.concatenate(BVP_tr)\n",
    "    ACC_tr  = np.concatenate(ACC_tr)\n",
    "    TEMP_tr = np.concatenate(TEMP_tr)\n",
    "    BEH_tr  = np.concatenate(BEH_tr)\n",
    "    y_tr    = np.concatenate(y_tr)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        WESADDataset(EDA_tr, BVP_tr, ACC_tr, TEMP_tr, BEH_tr, y_tr),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        WESADDataset(EDA_test, BVP_test, ACC_test, TEMP_test, BEH_test, y_test),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = EmotionModel(beh_dim=BEH_tr.shape[1]).to(device)\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.array([0,1,2]),\n",
    "        y=y_tr\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        weight=torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            EDA, BVP, ACC, TEMP, BEH, y = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(EDA, BVP, ACC, TEMP, BEH), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    preds, gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            EDA, BVP, ACC, TEMP, BEH, y = batch\n",
    "            out = model(\n",
    "                EDA.to(device), BVP.to(device),\n",
    "                ACC.to(device), TEMP.to(device),\n",
    "                BEH.to(device)\n",
    "            )\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            gts.extend(y.numpy())\n",
    "\n",
    "    print(classification_report(gts, preds, zero_division=0))\n",
    "    all_reports.append(classification_report(gts, preds, output_dict=True))\n",
    "\n",
    "print(\"\\nâœ… LOSO Validation Completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
